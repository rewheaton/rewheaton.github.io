{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rz0poNLMX8S"
   },
   "source": [
    "# Lab - Data Preprocessing\n",
    "\n",
    "## Lab Summary:\n",
    "In this lab we will be learning about NLP data preprocessing techniques, including Bag of Words, TF-IDF, and Document Similarity\n",
    "\n",
    "## Learning Outcomes:\n",
    "Upon completion of this lab, the student can:\n",
    "<ul>\n",
    "    <li>Compare cosine, Jaccard, and Euclidean similarity </li>\n",
    "    <li>Apply Python to apply pre-processing techniques, including Bag of Words, TF-IDF, and Document Similarity</li>\n",
    "    <li>Apply Python to test the similarity of 2 documents</li>\n",
    "</ul>\n",
    "\n",
    "## Import Packages and Classes (Initial)\n",
    "In this lab we will use these libraries:\n",
    "<ol>\n",
    "    <li> NLTK </li>\n",
    "    <li> Pandas </li>\n",
    "    <li> Matplotlib </li>\n",
    "    <li> Gensim </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiPqiK6lXHno"
   },
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "<b>Bag of Words</b> is a text modelling technique. Bag of words creates a vector, using the count of each word within a document.\n",
    "\n",
    "It is possible to generate a BoW algorithm without using a pre-created Python library.  We will review this method and apply it to some words.  Then, we'll do the same thing using the scikit-learn Python library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (3.10.3)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: click in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.3\n",
      "\u001b[2K    Uninstalling numpy-2.3.3:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.3\n",
      "\u001b[2K  Attempting uninstall: scipy━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.15.3[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.15.3:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.15.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [gensim]2m2/3\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk pandas matplotlib gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ybu5sUvX8-Ei"
   },
   "outputs": [],
   "source": [
    "def unique(sequence):\n",
    "    '''this function returns the vocabulary of words'''\n",
    "    seen = set()\n",
    "    return set(sequence) \n",
    "\n",
    "def vectorize(tokens):\n",
    "    ''' This function returns the bag of words representation. You need to use this function ahead'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "        #tokens.count(w) tells us the count of each word in the filtered_vocab in tokens\n",
    "        #filtered_vocab contains list of unique words after filtering stopwords and punctuation\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkKto_t39f3C",
    "outputId": "e6563680-008d-4180-80bd-9048fc98cf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'is', 'the', 'capital', 'of', 'france']\n",
      "['milan', 'is', 'the', 'fashion', 'capital', 'of', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Define some stopwords and special characters\n",
    "# We wish to ignore these in our BoW algorithm.\n",
    "stopwords=[\"to\",\"is\",\"a\",\"the\",\"of\"]\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "# Some sentences\n",
    "string1=\"Paris is the capital of France\"\n",
    "string2=\"Milan is the fashion capital of the world\"\n",
    "\n",
    "# Lowercasing is one of the most important steps in text preprocessing. \n",
    "# A particular word whether in lower or upper case means the same thing.\n",
    "# Convert sentences to lowercase.\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "# Split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "\n",
    "# Print the tokens and visually inspect them.\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq6g_Y1zr_A8"
   },
   "source": [
    "Now, we will find the <i>vocabulary</i> - the set of unique words in the corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSVJfDPE9f6_",
    "outputId": "1a1c7996-a47a-42aa-d140-ad93151ce335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital', 'fashion', 'france', 'is', 'milan', 'of', 'paris', 'the', 'world'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply our function \"unique\" as created previously:\n",
    "vocab=unique(tokens1+tokens2)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our vocabulary is the set of unique words in the corpus (across all documents).\n",
    "\n",
    "We also need to remove stopwords and special characters, as they add little or no meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDYRbZb59f9i",
    "outputId": "c4b83f9b-1950-4889-ff6e-c762b8875e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'capital', 'milan', 'fashion', 'france', 'world']\n"
     ]
    }
   ],
   "source": [
    "filtered_vocab=[]\n",
    "# filtered_vocab should contain the words from vocab that are not stopwords or special_char\n",
    "# It should contain all the meaningful words in all of the text items.\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "We have identified unique words that have meaning.\n",
    "\n",
    "Now, we will apply a technique called <i>vectorization</i>. This substitutes text with numbers.\n",
    "\n",
    "Vectorization is a critical step in NLP, because computers process numeric values.\n",
    "\n",
    "We will use our <code>vectorize</code> function that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkhVniRK9gAy",
    "outputId": "fba85ed5-1d36-4270-85d1-8771df98e65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'capital', 'milan', 'fashion', 'france', 'world']\n",
      "[1, 1, 0, 0, 1, 0]\n",
      "[0, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Remind ourselves what those unique, meaningful words were.\n",
    "print(filtered_vocab)\n",
    "\n",
    "# Convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "vector2=vectorize(tokens2)\n",
    "\n",
    "# Print the vectorized tokens.\n",
    "print(vector1)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itYF6KLmEhi5"
   },
   "source": [
    "# Bag of Words with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we do not have to create a new set of functions every time we wish to use the BoW model.\n",
    "\n",
    "Scikit-Learn has built-in modules that do this work for us.  \n",
    "\n",
    "Below, we'll put these libraries into practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHdZ5TdTW_FO",
    "outputId": "aa72aaa1-8ff0-432c-87c7-d85cbe380b8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweden is part of the geographical area of Fennoscandia',\n",
       " ' The climate is in general mild for its northerly latitude due to significant maritime influence',\n",
       " ' In spite of the high latitude, Sweden often has warm continental summers, being located in between the North Atlantic, the Baltic Sea, and vast Russia',\n",
       " ' The general climate and environment vary significantly from the south and north due to the vast latitudal difference, and much of Sweden has reliably cold and snowy winters',\n",
       " ' Southern Sweden is predominantly agricultural, while the north is heavily forested and includes a portion of the Scandinavian Mountains',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store your text into a variable.\n",
    "text = \"Sweden is part of the geographical area of Fennoscandia. \\\n",
    "The climate is in general mild for its northerly latitude due to \\\n",
    "significant maritime influence. In spite of the high latitude, \\\n",
    "Sweden often has warm continental summers, being located in \\\n",
    "between the North Atlantic, the Baltic Sea, and vast Russia. \\\n",
    "The general climate and environment vary significantly from the \\\n",
    "south and north due to the vast latitudal difference, and much \\\n",
    "of Sweden has reliably cold and snowy winters. Southern Sweden \\\n",
    "is predominantly agricultural, while the north is heavily forested \\\n",
    "and includes a portion of the Scandinavian Mountains.\"\n",
    "\n",
    "# Split the text into sentences and store them into a list, using the decimal as sentence separator.\n",
    "text = text.split('.')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhjO6YfGMX8f"
   },
   "source": [
    "#### sklearn CountVectorizer\n",
    "\n",
    "<code>CountVectorizer</code> provides a simple way to tokenize a collection of text documents, build a vocabulary of known words, and encode new documents using that vocabulary.\n",
    "\n",
    "<b>Steps to use CountVectorizer:</b>\n",
    "- Create an instance of the CountVectorizer class.\n",
    "- Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "- Call the fit_transform() function on one or more documents as needed to encode each as a vector.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xQbIfM4MX8f",
    "outputId": "5ace57d5-e2c2-471e-cb72-e3ae2a0be462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   agricultural  area  atlantic  baltic  climate  cold  continental  \\\n",
      "0             0     1         0       0        0     0            0   \n",
      "1             0     0         0       0        1     0            0   \n",
      "2             0     0         1       1        0     0            1   \n",
      "3             0     0         0       0        1     1            0   \n",
      "4             1     0         0       0        0     0            0   \n",
      "5             0     0         0       0        0     0            0   \n",
      "\n",
      "   difference  environment  fennoscandia  ...  snowy  south  southern  spite  \\\n",
      "0           0            0             1  ...      0      0         0      0   \n",
      "1           0            0             0  ...      0      0         0      0   \n",
      "2           0            0             0  ...      0      0         0      1   \n",
      "3           1            1             0  ...      1      1         0      0   \n",
      "4           0            0             0  ...      0      0         1      0   \n",
      "5           0            0             0  ...      0      0         0      0   \n",
      "\n",
      "   summers  sweden  vary  vast  warm  winters  \n",
      "0        0       1     0     0     0        0  \n",
      "1        0       0     0     0     0        0  \n",
      "2        1       1     0     1     1        0  \n",
      "3        0       1     1     1     0        1  \n",
      "4        0       1     0     0     0        0  \n",
      "5        0       0     0     0     0        0  \n",
      "\n",
      "[6 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in text)\n",
    " \n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4VCKtdl8NcN"
   },
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the steps in the previous example, create a dataframe of the vector of bi-grams in the following text:\n",
    "\n",
    "\"The Bag of Words (BoW) model is a foundational technique in natural language processing that converts text into numerical feature vectors by counting word occurrences. In Python, libraries like Scikit-learn provide tools such as CountVectorizer to transform a corpus of text into a sparse matrix of word counts. This representation is commonly used as input for machine learning models like Naive Bayes, logistic regression, or support vector machines to perform tasks such as text classification or sentiment analysis. While BoW ignores grammar and word order, it captures essential frequency information that can be highly effective in many NLP applications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bag words</th>\n",
       "      <th>bayes logistic</th>\n",
       "      <th>bow ignores</th>\n",
       "      <th>bow model</th>\n",
       "      <th>captures essential</th>\n",
       "      <th>classification sentiment</th>\n",
       "      <th>commonly used</th>\n",
       "      <th>converts text</th>\n",
       "      <th>corpus text</th>\n",
       "      <th>counting word</th>\n",
       "      <th>...</th>\n",
       "      <th>text sparse</th>\n",
       "      <th>tools countvectorizer</th>\n",
       "      <th>transform corpus</th>\n",
       "      <th>used input</th>\n",
       "      <th>vector machines</th>\n",
       "      <th>vectors counting</th>\n",
       "      <th>word counts</th>\n",
       "      <th>word occurrences</th>\n",
       "      <th>word order</th>\n",
       "      <th>words bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bag words  bayes logistic  bow ignores  bow model  captures essential  \\\n",
       "0          1               0            0          1                   0   \n",
       "1          0               0            0          0                   0   \n",
       "2          0               1            0          0                   0   \n",
       "3          0               0            1          0                   1   \n",
       "4          0               0            0          0                   0   \n",
       "\n",
       "   classification sentiment  commonly used  converts text  corpus text  \\\n",
       "0                         0              0              1            0   \n",
       "1                         0              0              0            1   \n",
       "2                         1              1              0            0   \n",
       "3                         0              0              0            0   \n",
       "4                         0              0              0            0   \n",
       "\n",
       "   counting word  ...  text sparse  tools countvectorizer  transform corpus  \\\n",
       "0              1  ...            0                      0                 0   \n",
       "1              0  ...            1                      1                 1   \n",
       "2              0  ...            0                      0                 0   \n",
       "3              0  ...            0                      0                 0   \n",
       "4              0  ...            0                      0                 0   \n",
       "\n",
       "   used input  vector machines  vectors counting  word counts  \\\n",
       "0           0                0                 1            0   \n",
       "1           0                0                 0            1   \n",
       "2           1                1                 0            0   \n",
       "3           0                0                 0            0   \n",
       "4           0                0                 0            0   \n",
       "\n",
       "   word occurrences  word order  words bow  \n",
       "0                 1           0          1  \n",
       "1                 0           0          0  \n",
       "2                 0           0          0  \n",
       "3                 0           1          0  \n",
       "4                 0           0          0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Here:\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_text = \"The Bag of Words (BoW) model is a foundational technique in natural language processing that converts text into numerical feature vectors by counting word occurrences. In Python, libraries like Scikit-learn provide tools such as CountVectorizer to transform a corpus of text into a sparse matrix of word counts. This representation is commonly used as input for machine learning models like Naive Bayes, logistic regression, or support vector machines to perform tasks such as text classification or sentiment analysis. While BoW ignores grammar and word order, it captures essential frequency information that can be highly effective in many NLP applications.\"\n",
    "bow_text = bow_text.split('.')\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(2,2), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in bow_text)\n",
    "pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnC-bALoMX8g"
   },
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
    "\n",
    "Like we did with Bag of Words, we'll demonstrate the long way of deriving TF-IDF, with a purpose of understanding how it works.\n",
    "\n",
    "After that, we'll use built-in functionality of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FUkPHjSTMX8j"
   },
   "outputs": [],
   "source": [
    "# A haiku:\n",
    "doc1 = 'When does summer begin, When'\n",
    "doc2 = 'The rain soothes my soul'\n",
    "doc3 = 'The winter is lovely but long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHPXmTLXp9GB",
    "outputId": "9cd233cf-e68c-4118-9d01-08ea78dc3eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'does', 'summer', 'begin,', 'When']\n",
      "['The', 'rain', 'soothes', 'my', 'soul']\n",
      "['The', 'winter', 'is', 'lovely', 'but', 'long']\n"
     ]
    }
   ],
   "source": [
    "# Split each of the three documents into tokens.\n",
    "bowDOC1 = doc1.split(' ')\n",
    "bowDOC2 = doc2.split(' ')\n",
    "bowDOC3 = doc3.split(' ')\n",
    "print(bowDOC1)\n",
    "print(bowDOC2)\n",
    "print(bowDOC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx4id3KUkOZG",
    "outputId": "d5b8a51e-fa36-4c39-b4d4-3c2c39894ec2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The',\n",
       " 'When',\n",
       " 'begin,',\n",
       " 'but',\n",
       " 'does',\n",
       " 'is',\n",
       " 'long',\n",
       " 'lovely',\n",
       " 'my',\n",
       " 'rain',\n",
       " 'soothes',\n",
       " 'soul',\n",
       " 'summer',\n",
       " 'winter'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember the vocabulary is the set of unique words from all the documents\n",
    "# Find the vocabulary by finding the unique words of bowDOC1, bowDOC2, and bowDOC3\n",
    "vocabulary = set(bowDOC1).union(set(bowDOC2)).union(set(bowDOC3))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJKemUT4uJ_w"
   },
   "source": [
    "Now, we have the vocabulary. \n",
    "\n",
    "Next, we vectorize using <code>TfidfVectorizer</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FMQPM75_MX8h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOL_YsXuJ3T5",
    "outputId": "150a8f3c-268d-4bfe-f125-4514a3bdf0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'lovely': 0, 'rain': 0, 'does': 0, 'my': 0, 'The': 0, 'begin,': 0, 'but': 0, 'long': 0, 'winter': 0, 'soul': 0, 'soothes': 0, 'summer': 0, 'When': 0}\n",
      "{'is': 0, 'lovely': 0, 'rain': 0, 'does': 1, 'my': 0, 'The': 0, 'begin,': 1, 'but': 0, 'long': 0, 'winter': 0, 'soul': 0, 'soothes': 0, 'summer': 1, 'When': 2}\n"
     ]
    }
   ],
   "source": [
    "# Find the vector for doc1\n",
    "\n",
    "# Create and print a dictionary from each word in the vocabulary.\n",
    "vectorA = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorA)\n",
    "\n",
    "# Using a for-loop, update the vector with the count of words in the first document.\n",
    "for word in bowDOC1:\n",
    "    vectorA[word] += 1\n",
    "print(vectorA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice - Vectorize doc1 and doc1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above example, vectorize doc2 and doc3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'lovely': 0, 'rain': 0, 'does': 0, 'my': 0, 'The': 0, 'begin,': 0, 'but': 0, 'long': 0, 'winter': 0, 'soul': 0, 'soothes': 0, 'summer': 0, 'When': 0}\n",
      "{'is': 0, 'lovely': 0, 'rain': 1, 'does': 0, 'my': 1, 'The': 1, 'begin,': 0, 'but': 0, 'long': 0, 'winter': 0, 'soul': 1, 'soothes': 1, 'summer': 0, 'When': 0}\n"
     ]
    }
   ],
   "source": [
    "# Find the vector for doc2\n",
    "# Create and print a dictionary from each word in the vocabulary.\n",
    "\n",
    "# Using a for-loop, update the vector with the count of words in the second document and print it.\n",
    "# Create and print a dictionary from each word in the vocabulary.\n",
    "vectorB = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorB)\n",
    "\n",
    "# Using a for-loop, update the vector with the count of words in the second document.\n",
    "for word in bowDOC2:\n",
    "    vectorB[word] += 1\n",
    "print(vectorB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'lovely': 0, 'rain': 0, 'does': 0, 'my': 0, 'The': 0, 'begin,': 0, 'but': 0, 'long': 0, 'winter': 0, 'soul': 0, 'soothes': 0, 'summer': 0, 'When': 0}\n",
      "{'is': 1, 'lovely': 1, 'rain': 0, 'does': 0, 'my': 0, 'The': 1, 'begin,': 0, 'but': 1, 'long': 1, 'winter': 1, 'soul': 0, 'soothes': 0, 'summer': 0, 'When': 0}\n"
     ]
    }
   ],
   "source": [
    "# Find the vector for doc3\n",
    "# Create and print a dictionary from each word in the vocabulary.\n",
    "\n",
    "# Using a for-loop, update the vector with the count of words in the third document and print it.\n",
    "vectorC = dict.fromkeys(vocabulary, 0)\n",
    "print(vectorC)\n",
    "\n",
    "# Using a for-loop, update the vector with the count of words in the second document.\n",
    "for word in bowDOC3:\n",
    "    vectorC[word] += 1\n",
    "print(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xvih1cWIurIL"
   },
   "source": [
    "# Final step: Produce the Term Frequency and Inverse Term Frequency\n",
    "\n",
    "TFIDF consists of 2 steps: finding the TF and the IDF. The final result is just the product of TF and IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bACWMbCJJ3cU"
   },
   "outputs": [],
   "source": [
    "# This function computes term frequency\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords) # Finds the length of list Bag of Words\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount) # Find term frequency\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PbLptcH0J3e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.0,\n",
       " 'lovely': 0.0,\n",
       " 'rain': 0.0,\n",
       " 'does': 0.2,\n",
       " 'my': 0.0,\n",
       " 'The': 0.0,\n",
       " 'begin,': 0.2,\n",
       " 'but': 0.0,\n",
       " 'long': 0.0,\n",
       " 'winter': 0.0,\n",
       " 'soul': 0.0,\n",
       " 'soothes': 0.0,\n",
       " 'summer': 0.2,\n",
       " 'When': 0.4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frquency for doc1\n",
    "tfA = computeTF(vectorA, bowDOC1)\n",
    "tfA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice - Compute the Term Frequency for doc2 and doc3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the example above, compute the Term Frequencies for doc2 and doc3.\n",
    "\n",
    "Store the results of each computation in variables called tfB and tfC, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here.\n",
    "# tfB:\n",
    "tfB = computeTF(vectorB, bowDOC2)\n",
    "\n",
    "# tfC:\n",
    "tfC = computeTF(vectorC, bowDOC3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.0,\n",
       " 'lovely': 0.0,\n",
       " 'rain': 0.2,\n",
       " 'does': 0.0,\n",
       " 'my': 0.2,\n",
       " 'The': 0.2,\n",
       " 'begin,': 0.0,\n",
       " 'but': 0.0,\n",
       " 'long': 0.0,\n",
       " 'winter': 0.0,\n",
       " 'soul': 0.2,\n",
       " 'soothes': 0.2,\n",
       " 'summer': 0.0,\n",
       " 'When': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your work:\n",
    "tfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.16666666666666666,\n",
       " 'lovely': 0.16666666666666666,\n",
       " 'rain': 0.0,\n",
       " 'does': 0.0,\n",
       " 'my': 0.0,\n",
       " 'The': 0.16666666666666666,\n",
       " 'begin,': 0.0,\n",
       " 'but': 0.16666666666666666,\n",
       " 'long': 0.16666666666666666,\n",
       " 'winter': 0.16666666666666666,\n",
       " 'soul': 0.0,\n",
       " 'soothes': 0.0,\n",
       " 'summer': 0.0,\n",
       " 'When': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Inverse Document Frequency function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "S86vGfDTLa3A"
   },
   "outputs": [],
   "source": [
    "# Function to compute inverse document frequency\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IH4ZKdIeLa9I",
    "outputId": "496947a3-3344-4b64-af27-94995a2b793f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1.0986122886681098,\n",
       " 'lovely': 1.0986122886681098,\n",
       " 'rain': 1.0986122886681098,\n",
       " 'does': 1.0986122886681098,\n",
       " 'my': 1.0986122886681098,\n",
       " 'The': 0.4054651081081644,\n",
       " 'begin,': 1.0986122886681098,\n",
       " 'but': 1.0986122886681098,\n",
       " 'long': 1.0986122886681098,\n",
       " 'winter': 1.0986122886681098,\n",
       " 'soul': 1.0986122886681098,\n",
       " 'soothes': 1.0986122886681098,\n",
       " 'summer': 1.0986122886681098,\n",
       " 'When': 1.0986122886681098}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = computeIDF([vectorA, vectorB, vectorC])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, calculate TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tUj_kJg2La-_"
   },
   "outputs": [],
   "source": [
    "# TF-IDF is calculated by multiplying tf * idf for each word.\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CxOfwqfMLbBV"
   },
   "outputs": [],
   "source": [
    "# Find the TF-IDF for 3 documents and represent them in a data frame\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "tfidfC = computeTFIDF(tfC, idfs)\n",
    "\n",
    "df = pd.DataFrame([tfidfA, tfidfB, tfidfC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "gT09wWKVLbEA",
    "outputId": "b35611ce-bf44-4d19-866a-b6a75af9f7ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>lovely</th>\n",
       "      <th>rain</th>\n",
       "      <th>does</th>\n",
       "      <th>my</th>\n",
       "      <th>The</th>\n",
       "      <th>begin,</th>\n",
       "      <th>but</th>\n",
       "      <th>long</th>\n",
       "      <th>winter</th>\n",
       "      <th>soul</th>\n",
       "      <th>soothes</th>\n",
       "      <th>summer</th>\n",
       "      <th>When</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.439445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         is    lovely      rain      does        my       The    begin,  \\\n",
       "0  0.000000  0.000000  0.000000  0.219722  0.000000  0.000000  0.219722   \n",
       "1  0.000000  0.000000  0.219722  0.000000  0.219722  0.081093  0.000000   \n",
       "2  0.183102  0.183102  0.000000  0.000000  0.000000  0.067578  0.000000   \n",
       "\n",
       "        but      long    winter      soul   soothes    summer      When  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.219722  0.439445  \n",
       "1  0.000000  0.000000  0.000000  0.219722  0.219722  0.000000  0.000000  \n",
       "2  0.183102  0.183102  0.183102  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW9I9Q6-rsS2"
   },
   "source": [
    "# Use Scikit-Learn to Calculate TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_OlOIVtNCU3",
    "outputId": "041af1dd-367b-4f98-a321-74125ca5385a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['begin', 'but', 'does', 'is', 'long', 'lovely', 'my', 'rain',\n",
       "       'soothes', 'soul', 'summer', 'the', 'when', 'winter'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit documents into the vectorizer\n",
    "vectors = vectorizer.fit_transform([doc1, doc2, doc3])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print vectors in a readable format\n",
    "print(feature_names) # prints the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      begin       but      does        is      long    lovely        my  \\\n",
      "0  0.377964  0.000000  0.377964  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.467351   \n",
      "2  0.000000  0.423394  0.000000  0.423394  0.423394  0.423394  0.000000   \n",
      "\n",
      "       rain   soothes      soul    summer       the      when    winter  \n",
      "0  0.000000  0.000000  0.000000  0.377964  0.000000  0.755929  0.000000  \n",
      "1  0.467351  0.467351  0.467351  0.000000  0.355432  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.322002  0.000000  0.423394  \n"
     ]
    }
   ],
   "source": [
    "# Our vectors are \"sparse\".  We eventually wish to transform our TF-IDF vectors\n",
    "# into a data frame, which will require us to transform them into \"dense.\"\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "# Print the tfidf vectors\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik4A14RiNcg_"
   },
   "source": [
    "Notice that the values slightly differ from our own calculations.\n",
    "\n",
    "This is because sklearn uses a slightly different implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice - scikit-Learn TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the examples above, create a dataframe of the dense vectors generated from the following paragraph, using scikit-learn's TfidfVectorizer function.\n",
    "\n",
    "<b>Use this paragraph:</b>\n",
    "\n",
    "The TF-IDF vectorization process transforms text data into numerical features by assigning weights based on how important a word is to a document relative to a collection of documents. Unlike the Bag of Words approach, which simply counts word occurrences, TF-IDF down-weights common words that appear in many documents and emphasizes more distinctive terms. Both methods convert text into sparse matrices, but TF-IDF captures more nuanced information about word significance. As a result, TF-IDF often leads to better performance in machine learning tasks where understanding word relevance is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here:\n",
    "# Store the text into a variable and split it into sentences, using the \".\" character.\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = 'The TF-IDF vectorization process transforms text data into numerical features by assigning weights based on how important a word is to a document relative to a collection of documents. Unlike the Bag of Words approach, which simply counts word occurrences, TF-IDF down-weights common words that appear in many documents and emphasizes more distinctive terms. Both methods convert text into sparse matrices, but TF-IDF captures more nuanced information about word significance. As a result, TF-IDF often leads to better performance in machine learning tasks where understanding word relevance is important.'\n",
    "sentences = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': 'The TF-IDF vectorization process transforms text data into numerical features by assigning weights based on how important a word is to a document relative to a collection of documents',\n",
       " 'doc2': 'Unlike the Bag of Words approach, which simply counts word occurrences, TF-IDF down-weights common words that appear in many documents and emphasizes more distinctive terms',\n",
       " 'doc3': 'Both methods convert text into sparse matrices, but TF-IDF captures more nuanced information about word significance',\n",
       " 'doc4': 'As a result, TF-IDF often leads to better performance in machine learning tasks where understanding word relevance is important'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store each sentence in documents.\n",
    "\n",
    "sentence_dict = {}\n",
    "for i, sentence in enumerate(sentences):\n",
    "    if sentence.strip():  # Avoid adding empty sentences\n",
    "        sentence_dict[f'doc{i+1}'] = sentence.strip()\n",
    "\n",
    "sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tfidf vectorization for each document.\n",
    "# Define a vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit documents into the vectorizer\n",
    "sentence_dict_vectors = vectorizer.fit_transform(sentence_dict.values())\n",
    "sentence_dict_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      about       and    appear  approach        as  assigning       bag  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000   0.211875  0.000000   \n",
      "1  0.000000  0.201839  0.201839  0.201839  0.000000   0.000000  0.201839   \n",
      "2  0.270352  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.255627   0.000000  0.000000   \n",
      "\n",
      "      based    better      both  ...        to  transforms  understanding  \\\n",
      "0  0.211875  0.000000  0.000000  ...  0.334090    0.211875       0.000000   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000    0.000000       0.000000   \n",
      "2  0.000000  0.000000  0.270352  ...  0.000000    0.000000       0.000000   \n",
      "3  0.000000  0.255627  0.000000  ...  0.201539    0.000000       0.255627   \n",
      "\n",
      "     unlike  vectorization   weights     where     which      word     words  \n",
      "0  0.000000       0.211875  0.167045  0.000000  0.000000  0.110565  0.000000  \n",
      "1  0.201839       0.000000  0.159132  0.000000  0.201839  0.105328  0.403678  \n",
      "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.141081  0.000000  \n",
      "3  0.000000       0.000000  0.000000  0.255627  0.000000  0.133397  0.000000  \n",
      "\n",
      "[4 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "# Store the vectorization in a dataframe format and display or print it.\n",
    "dense = sentence_dict_vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "# Print the tfidf vectors\n",
    "df = pd.DataFrame(denselist, columns=sentence_dict_feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aVOL_8hTdjs"
   },
   "source": [
    "# Document Similarity\n",
    "\n",
    "Text Similarity determines how \"close\" to each other are multiple text documents.\n",
    "\n",
    "Similarity can be in terms of both context and meaning.\n",
    "\n",
    "Various text similarity metrics exist, including:\n",
    "\n",
    "1. Cosine similarity\n",
    "2. Jaccard similarity\n",
    "3. Euclidean similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFGUEGSW-GKe"
   },
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar 2 documents are.\n",
    "\n",
    "Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.\n",
    "\n",
    "The smaller the angle, higher the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "__NqSdhTTc0Q"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"Brazil won the Football world cup five times\" \n",
    "doc_2 = \"Italy comes after Brazil in that regard\" \n",
    "\n",
    "# Find the bag of word vector representation\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6X3PLlBBPOW"
   },
   "source": [
    "Now, calculate the cosine similarity between documents using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "SkZOW5YmTVes",
    "outputId": "ebbeb2b7-ff7f-46f2-9eb9-774aa5587150"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.133631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_1  1.000000  0.133631\n",
       "doc_2  0.133631  1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "\n",
    "# Display the dataframe that shows the similarity of the two documents.\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you have more than two documents?\n",
    "\n",
    "How many comparisons would you need to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GHXHDpbvVyxE"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"Sweden is in Scandanavia\" \n",
    "doc_2 = \"Denmark is a neighbor of Sweden\" \n",
    "doc_3 = \"Norway and Denmark are close by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "xADVY4iu9MJP",
    "outputId": "3a79b1df-c47c-473a-e233-8efab6082a48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_1  1.000000  0.447214\n",
       "doc_2  0.447214  1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Between the first and second:\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_2])\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "GRlWqIZY9p9T",
    "outputId": "b5409884-49c5-492b-87f7-91b8b2015263"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.182574</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "doc_2  1.000000  0.182574\n",
       "doc_3  0.182574  1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Between the second and third:\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_2, doc_3])\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_2','doc_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "jaWsQF6Y9wDD",
    "outputId": "3100c596-d2da-4409-81a2-d7660989f398"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1\n",
       "doc_1  1.0  0.0\n",
       "doc_3  0.0  1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Between first and third:\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc_1, doc_3])\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc_1','doc_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cosine similarity between the sentences in this paragraph:\n",
    "\n",
    "<b>Use the sentences from this paragraph:</b>\n",
    "\n",
    "Cosine similarity measures the angle between two vectors. It captures directional similarity, not magnitude differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here:\n",
    "# Store your text into a variable.\n",
    "cosine_text = \"Cosine similarity measures the angle between two vectors. It captures directional similarity, not magnitude differences.\"\n",
    "\n",
    "# Split the text into sentences and store them into a list, using the decimal as sentence separator.\n",
    "cosine_sentences = cosine_text.split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.133631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "doc1  1.000000  0.133631\n",
       "doc2  0.133631  1.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign the first element of text to one variable and the second element of the text to a different variable.\n",
    "\n",
    "# Compare the first and second documents:\n",
    "doc1 = cosine_sentences[0].strip()\n",
    "doc2 = cosine_sentences[1].strip()\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1))\n",
    "Count_data = CountVec.fit_transform(sentence for sentence in [doc1, doc2])\n",
    "cosine_similarity_matrix = cosine_similarity(Count_data)\n",
    "pd.DataFrame(cosine_similarity_matrix,['doc1','doc2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "andfzSnG-Ey8"
   },
   "source": [
    "# Jaccard Similarity\n",
    "Jaccard Similarity is also known as the Jaccard index and Intersection over Union.\n",
    "\n",
    "Jaccard is used to determine the similarity between two text document in terms of their context.\n",
    "\n",
    "Similarity is in terms of how many common words are exist over total words\n",
    "\n",
    "![Image](jaccard.png)\n",
    "\n",
    "Reference: https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "9HxZXmfs91Ug"
   },
   "outputs": [],
   "source": [
    "# Start with two docs.\n",
    "doc1 = 'A is the brother of B'\n",
    "doc2 = 'B is the friend of C who is not a brother of A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GkzcIHzpCLGN"
   },
   "outputs": [],
   "source": [
    "# Convert them to lower case for preprocessing.\n",
    "doc1 = doc1.lower()\n",
    "doc2 = doc2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "WDGGxg98Doc3"
   },
   "outputs": [],
   "source": [
    "# Split into tokens. Make sure that you have no duplicates. You might want to use sets for this purpose.\n",
    "doc1 = set(doc1.split())\n",
    "doc2 = set(doc2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "TC9UGMnGC59r"
   },
   "outputs": [],
   "source": [
    "# Find common words from the 2 documents.\n",
    "intersection = doc1.intersection(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "PbEB770eDzZJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'brother', 'c', 'friend', 'is', 'not', 'of', 'the', 'who'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the vocabulary - unique words in both documents\n",
    "union = doc1.union(doc2)\n",
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcfjZWD5EEjw",
    "outputId": "c86da5f6-1610-451b-81f9-141a1fc058b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate jaccard similarity\n",
    "float(len(intersection)) / len(union)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_Bag_of_Words,_TFIDF,_Document_Similarity.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
