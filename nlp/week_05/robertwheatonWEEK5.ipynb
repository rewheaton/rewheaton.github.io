{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rz0poNLMX8S"
   },
   "source": [
    "# Lab - NLP Pipeline\n",
    "\n",
    "## Lab Summary:\n",
    "In this lab we will be discussing NLP pipelines.\n",
    "\n",
    "## Lab Goal:\n",
    "Upon completion of this lab, the student should be able to:\n",
    "<ul>\n",
    "    <li> Design a simple NLP pipeline for a problem statement </li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "## Import Packages and Classes (Initial)\n",
    "We will be using the following libraries:\n",
    "<ol>\n",
    "    <li> NLTK </li>\n",
    "    <li> Pandas </li>\n",
    "    <li> Matplotlib </li>\n",
    "    <li> Gensim </li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (3.10.6)\n",
      "Requirement already satisfied: gensim in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (4.3.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from gensim) (7.3.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.7.2 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk pandas matplotlib gensim scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiPqiK6lXHno"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The twenty newsgroup dataset from the sklearn library is used for this lab.\n",
    "\n",
    "\n",
    "![Image](https://research.cs.aalto.fi/pml/software/ne/20newsgroups_wtsne.png)\n",
    "\n",
    "The dataset contains over 18,000 messages with assigned topic labels and is split into train and test datasets. The training dataset contains around 11,300 labelled messages and the test dataset contains 7,500 messages without labels. The task is to predict these labels.\n",
    "\n",
    "The categories include:\n",
    "\n",
    " -  'alt.atheism'\n",
    " -  'comp.graphics'\n",
    " -  'comp.os.ms-windows.misc'\n",
    " -  'comp.sys.ibm.pc.hardware'\n",
    " -  'comp.sys.mac.hardware'\n",
    " -  'comp.windows.x'\n",
    " -  'misc.forsale'\n",
    " -  'rec.autos'\n",
    " -  'rec.motorcycles'\n",
    " -  'rec.sport.baseball'\n",
    " -  'rec.sport.hockey'\n",
    " -  'sci.crypt'\n",
    " -  'sci.electronics'\n",
    " -  'sci.med'\n",
    " -  'sci.space'\n",
    " -  'soc.religion.christian'\n",
    " -  'talk.politics.guns'\n",
    " -  'talk.politics.mideast'\n",
    " -  'talk.politics.misc'\n",
    " -  'talk.religion.misc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset \"fetch_20newsgroups\" and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXVgpbJ79ge1",
    "outputId": "73c4803c-0be1-429e-c821-4a09771bc9a4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDEs90E3BdUi"
   },
   "source": [
    "See all the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpqRz1gfBga1",
    "outputId": "796fa400-3e2e-4710-cd8a-9438b179f36c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIyKr-Au-UuI",
    "outputId": "ca196512-a9e8-4545-edfd-8a82d9d773ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "twenty_train.target_names #prints all the categories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLwPl8c6B-tR"
   },
   "source": [
    "### Apply <b>Bag of Words</b>.\n",
    "\n",
    "We learned about the Bag of Words model in a previous discussion.\n",
    "\n",
    "<b><code>CountVectorizer</code></b> calculates the BoW model automatically with a few line of code.\n",
    "\n",
    "What <code>CountVectorizer</code> does:\n",
    "\n",
    "<li>Converts a collection of text documents into a matrix of token counts.</li>\n",
    "<li>Tokenizes the text (splits into words)</li>\n",
    "<li>Builds a vocabulary of known words</li>\n",
    "<li>Encodes each document as a vector where each element counts how many times a word appears in that document.</li>\n",
    "\n",
    "Reference: https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phq5wEyz-Yjf",
    "outputId": "c0ac8f88-7124-4283-8041-2208d9dd6d86"
   },
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><code>fit_transform</code></b> does two things:\n",
    "1. <i>fit</i>: Learns the vocabulary from twenty_train.data (which is a list of text documents).\n",
    "2. <i>transform</i>: Transforms the documents into a sparse matrix where rows correspond to documents and columns to words from the vocabulary. Each value in the matrix is the count of a word in a document.\n",
    "\n",
    "We'll call the .shape method on the X_train_counts object to see the shape of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data to CountVectoriser and store it in a variable X_train_counts, and print the shape \n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PnsROpvCbt6"
   },
   "source": [
    "### Apply <b>TF-IDF</b>.\n",
    "\n",
    "We also learned the long way of calculating TF-IDF.  \n",
    "\n",
    "This can also be done with a few lines of code, using <code>TfidfTransformer</code> from <code>feature_extraction.text</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwPOkzCy-btL",
    "outputId": "77a2301e-8f7f-4a75-f564-81d4ee7fd214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TfidfTransformer object.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit the data to TfidfTransformer and store it in a variable X_train_tfidf, and print the shape \n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaP1JeakDrEt"
   },
   "source": [
    "### <b>Naive Bayes</b> classifier, also available in the sklearn library.\n",
    "\n",
    "#### What is Naive Bayes?\n",
    "\n",
    "Naive Bayes classifies text into categories by:\n",
    "1. Learning how often each word appears in each category from your training data.\n",
    "2. Calculating the probabilities that a new document belongs to each category based on the words it contains.\n",
    "3. Selecting the category with the highest probability.\n",
    "\n",
    "\"Naive\" because it assumes every word in the document is independent of the others. This is not a valid assumption, but the system works well in practice.\n",
    "\n",
    "##### Why Naive Bayes?\n",
    "\n",
    "<li><b>Fast</b>: Trains quickly even on large datasets.</li>\n",
    "<li><b>Simple</b>: Easy to understand and implement.</li>\n",
    "<li><b>Surprisingly good</b>: Works well on many text classification problems.</li>\n",
    "\n",
    "##### Common Use Cases for Naive Bayes in NLP:\n",
    "<li>Spam detection</li>\n",
    "<li>Sentiment analysis</li>\n",
    "<li>News categorization</li>\n",
    "<li>Language identification</li>\n",
    "\n",
    "Silly video describing Naive Bayes in more detail: \n",
    "https://www.youtube.com/watch?v=O2L2Uv9pdDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4UHNVngL-zBX"
   },
   "outputs": [],
   "source": [
    "# Import Multinomial Naive Bayes module and create a fit() object.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction on the Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.52      0.63       319\n",
      "           comp.graphics       0.81      0.65      0.72       389\n",
      " comp.os.ms-windows.misc       0.82      0.65      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.67      0.78      0.72       392\n",
      "   comp.sys.mac.hardware       0.86      0.77      0.81       385\n",
      "          comp.windows.x       0.89      0.75      0.82       395\n",
      "            misc.forsale       0.93      0.69      0.80       390\n",
      "               rec.autos       0.85      0.92      0.88       396\n",
      "         rec.motorcycles       0.94      0.93      0.93       398\n",
      "      rec.sport.baseball       0.92      0.90      0.91       397\n",
      "        rec.sport.hockey       0.89      0.97      0.93       399\n",
      "               sci.crypt       0.59      0.97      0.74       396\n",
      "         sci.electronics       0.84      0.60      0.70       393\n",
      "                 sci.med       0.92      0.74      0.82       396\n",
      "               sci.space       0.84      0.89      0.87       394\n",
      "  soc.religion.christian       0.44      0.98      0.61       398\n",
      "      talk.politics.guns       0.64      0.94      0.76       364\n",
      "   talk.politics.mideast       0.93      0.91      0.92       376\n",
      "      talk.politics.misc       0.96      0.42      0.58       310\n",
      "      talk.religion.misc       0.97      0.14      0.24       251\n",
      "\n",
      "                accuracy                           0.77      7532\n",
      "               macro avg       0.83      0.76      0.76      7532\n",
      "            weighted avg       0.82      0.77      0.77      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import some measurement libraries:\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load some Test data.\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Make predictions\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(twenty_test.target, predicted):.2f}\")\n",
    "print(classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look back: What did we just do?\n",
    "\n",
    "The table above shows the performance of our Naive Bayes Classifier, when using Training data to classify data in the Test data.\n",
    "\n",
    "Accuracy is a measure of the correct classifications versus all classifications.  \n",
    "\n",
    "<b>Accuracy = 0.77</b> tells us that 77% of the time, the Bayes classifier correctly classified the test data into their respective news categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hituqKJEPN-"
   },
   "source": [
    "The ML process traditionally involves a common set of steps, like the ones above.\n",
    "\n",
    "<b>Pipelines</b> streamline those steps and help you build models more quickly.\n",
    "\n",
    "For example, preprocessing often starts a ML model, followed by some transformation, followed by the machine learning algorithm.  \n",
    "\n",
    "The Pipeline function is available in the Sklearn library. With the Pipeline function, you can perform all the key steps at once.  In this example, these three steps:\n",
    "\n",
    "- ('vect', CountVectorizer())\n",
    "- ('tfidf', TfidfTransformer())\n",
    "- ('clf', MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "R5UzGb-u-6-v"
   },
   "outputs": [],
   "source": [
    "# Import the Pipeline function and create a Pipeline object.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qQ74i9ks_ARs"
   },
   "outputs": [],
   "source": [
    "# Use the pipeline object. Fit the Training data agains the Target.\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CKYlHUF_jwx",
    "outputId": "4edad5fa-544a-4f72-fd70-c3a6350da894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.52      0.63       319\n",
      "           comp.graphics       0.81      0.65      0.72       389\n",
      " comp.os.ms-windows.misc       0.82      0.65      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.67      0.78      0.72       392\n",
      "   comp.sys.mac.hardware       0.86      0.77      0.81       385\n",
      "          comp.windows.x       0.89      0.75      0.82       395\n",
      "            misc.forsale       0.93      0.69      0.80       390\n",
      "               rec.autos       0.85      0.92      0.88       396\n",
      "         rec.motorcycles       0.94      0.93      0.93       398\n",
      "      rec.sport.baseball       0.92      0.90      0.91       397\n",
      "        rec.sport.hockey       0.89      0.97      0.93       399\n",
      "               sci.crypt       0.59      0.97      0.74       396\n",
      "         sci.electronics       0.84      0.60      0.70       393\n",
      "                 sci.med       0.92      0.74      0.82       396\n",
      "               sci.space       0.84      0.89      0.87       394\n",
      "  soc.religion.christian       0.44      0.98      0.61       398\n",
      "      talk.politics.guns       0.64      0.94      0.76       364\n",
      "   talk.politics.mideast       0.93      0.91      0.92       376\n",
      "      talk.politics.misc       0.96      0.42      0.58       310\n",
      "      talk.religion.misc       0.97      0.14      0.24       251\n",
      "\n",
      "                accuracy                           0.77      7532\n",
      "               macro avg       0.83      0.76      0.76      7532\n",
      "            weighted avg       0.82      0.77      0.77      7532\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how well the prediction fits the test data.\n",
    "import numpy as np\n",
    "\n",
    "# Recreate the test dataset.\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Calculate the prediction on the Test data.\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "# Print the performance measures:\n",
    "print(f\"Accuracy: {accuracy_score(twenty_test.target, predicted):.2f}\")\n",
    "print(classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "\n",
    "# Another way to calculate the accuracy:\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify a new headline\n",
    "\n",
    "Classify new headlines with the method used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: talk.politics.misc\n"
     ]
    }
   ],
   "source": [
    "# Store the new headline into a variable.\n",
    "headline = [\"Celebrity faces charges over gambling\"]\n",
    "\n",
    "# Predict the category using text_clf.predict\n",
    "predicted_category_index = text_clf.predict(headline)[0]\n",
    "\n",
    "# Retrieve and print the human-readable category name\n",
    "predicted_category_name = twenty_train.target_names[predicted_category_index]\n",
    "print(f\"Predicted category: {predicted_category_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "Classify a new article headline using the model we created with the pipeline.\n",
    "\n",
    "Using the following headline, predict the category it belongs to.\n",
    "\n",
    "#### Headline: <i>NASA is planning a new mission to Jupiter.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: sci.space\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "# Store the new headline into a variable.\n",
    "headline = [\"NASA is planning a new mission to Jupiter.\"]\n",
    "\n",
    "# Predict the category using text_clf.predict\n",
    "predicted_category_index = text_clf.predict(headline)[0]\n",
    "\n",
    "# Retrieve and print the human-readable category name\n",
    "predicted_category_name = twenty_train.target_names[predicted_category_index]\n",
    "print(f\"Predicted category: {predicted_category_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQMK-DCGEgAY"
   },
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "A Support Vector Machine (SVM) is a different kind of supervised machine learning model. \n",
    "\n",
    "SVM identifies the boundary - the support vector - that best separates two categories. \n",
    "\n",
    "SVM then classifies observations according to their location relative to that support vector.\n",
    "\n",
    "![Image](https://miro.medium.com/max/921/1*06GSco3ItM3gwW2scY6Tmg.png)\n",
    "\n",
    "More about SVM: https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3jZGkToEyvm"
   },
   "source": [
    "### SVM Pipeline Objects:\n",
    "\n",
    "<code>CountVectorizer()</code>\n",
    "\n",
    "<code>TfidfTransformer()</code>\n",
    "\n",
    "<code>SGDClassifier()</code>\n",
    "\n",
    "### SGDClassifier Hyperparameters:\n",
    "\n",
    "We'll use the following hyperparameters for our example.\n",
    "\n",
    "- ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Headlines with SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8_APvFZk_mHd"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Store the pipeline object in a variable 'text_clf_svm':\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM0GD9SxFL0_",
    "outputId": "5f94c40e-8c81-42c5-aa14-5dbe28f7b100"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:726: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.73      0.71      0.72       319\n",
      "           comp.graphics       0.78      0.72      0.75       389\n",
      " comp.os.ms-windows.misc       0.73      0.78      0.75       394\n",
      "comp.sys.ibm.pc.hardware       0.74      0.67      0.70       392\n",
      "   comp.sys.mac.hardware       0.81      0.83      0.82       385\n",
      "          comp.windows.x       0.84      0.76      0.80       395\n",
      "            misc.forsale       0.84      0.90      0.87       390\n",
      "               rec.autos       0.91      0.90      0.90       396\n",
      "         rec.motorcycles       0.93      0.96      0.95       398\n",
      "      rec.sport.baseball       0.88      0.90      0.89       397\n",
      "        rec.sport.hockey       0.88      0.99      0.93       399\n",
      "               sci.crypt       0.84      0.96      0.90       396\n",
      "         sci.electronics       0.83      0.62      0.71       393\n",
      "                 sci.med       0.87      0.86      0.87       396\n",
      "               sci.space       0.84      0.96      0.90       394\n",
      "  soc.religion.christian       0.76      0.94      0.84       398\n",
      "      talk.politics.guns       0.70      0.92      0.80       364\n",
      "   talk.politics.mideast       0.90      0.93      0.92       376\n",
      "      talk.politics.misc       0.89      0.55      0.68       310\n",
      "      talk.religion.misc       0.85      0.41      0.55       251\n",
      "\n",
      "                accuracy                           0.82      7532\n",
      "               macro avg       0.83      0.81      0.81      7532\n",
      "            weighted avg       0.83      0.82      0.82      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the fit function on the twenty_train data that we created previously.\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Predict labels for twenty_test data.\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "# Print the performance measures:\n",
    "print(f\"Accuracy: {accuracy_score(twenty_test.target, predicted_svm):.2f}\")\n",
    "print(classification_report(twenty_test.target, predicted_svm, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict our old headline using SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: misc.forsale\n"
     ]
    }
   ],
   "source": [
    "# Store the new headline into a variable.\n",
    "headline = [\"Celebrity faces charges over gambling\"]\n",
    "\n",
    "# Predict the category using text_clf.predict\n",
    "predicted_category_index = text_clf_svm.predict(headline)[0]\n",
    "\n",
    "# Retrieve and print the human-readable category name\n",
    "predicted_category_name = twenty_train.target_names[predicted_category_index]\n",
    "print(f\"Predicted category: {predicted_category_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: What happened?\n",
    "\n",
    "The headline category from SVM is different from Naive Bayes. What are some reasons why this might be?\n",
    "\n",
    "Which one do you think is a better classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdPFh8hZE92y"
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are popular for classification and prediction. \n",
    "\n",
    "A Decision tree is a flowchart-like tree structure, where:\n",
    "\n",
    "1. Each internal node denotes a test on an attribute\n",
    "2. Each branch represents an outcome of the test\n",
    "3. Each leaf node (terminal node) holds a class label.\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/decision-tree/\n",
    "\n",
    "![Image](https://media.geeksforgeeks.org/wp-content/cdn-uploads/Decision_Tree-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Pipeline\n",
    "\n",
    "Our convenient Pipeline can do the same work we have done before with just a quick change to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uhmAc_FG_MDc"
   },
   "outputs": [],
   "source": [
    "# Create the Decision Tree pipeline, starting with Vectorizing and tranformation.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Pipeline: Countvectorizer, tfidftransformer, Decision tree classifier\n",
    "text_clf_decisiontree = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzBo2qi4_1LZ",
    "outputId": "0fef256f-df5b-4290-e621-8e745583c25a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.50      0.47      0.49       319\n",
      "           comp.graphics       0.41      0.43      0.42       389\n",
      " comp.os.ms-windows.misc       0.52      0.54      0.53       394\n",
      "comp.sys.ibm.pc.hardware       0.44      0.42      0.43       392\n",
      "   comp.sys.mac.hardware       0.53      0.58      0.55       385\n",
      "          comp.windows.x       0.47      0.45      0.46       395\n",
      "            misc.forsale       0.62      0.73      0.67       390\n",
      "               rec.autos       0.62      0.59      0.61       396\n",
      "         rec.motorcycles       0.74      0.74      0.74       398\n",
      "      rec.sport.baseball       0.55      0.55      0.55       397\n",
      "        rec.sport.hockey       0.63      0.67      0.65       399\n",
      "               sci.crypt       0.75      0.71      0.73       396\n",
      "         sci.electronics       0.32      0.35      0.33       393\n",
      "                 sci.med       0.56      0.44      0.49       396\n",
      "               sci.space       0.64      0.63      0.63       394\n",
      "  soc.religion.christian       0.69      0.69      0.69       398\n",
      "      talk.politics.guns       0.49      0.63      0.55       364\n",
      "   talk.politics.mideast       0.78      0.61      0.69       376\n",
      "      talk.politics.misc       0.41      0.38      0.40       310\n",
      "      talk.religion.misc       0.32      0.33      0.32       251\n",
      "\n",
      "                accuracy                           0.55      7532\n",
      "               macro avg       0.55      0.55      0.55      7532\n",
      "            weighted avg       0.56      0.55      0.55      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data using the decision tree classifier:\n",
    "text_clf_decisiontree = text_clf_decisiontree.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_decisiontree = text_clf_decisiontree.predict(twenty_test.data)\n",
    "\n",
    "# Print the performance measures:\n",
    "print(f\"Accuracy: {accuracy_score(twenty_test.target, predicted_decisiontree):.2f}\")\n",
    "print(classification_report(twenty_test.target, predicted_decisiontree, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree was far less accurate.\n",
    "\n",
    "SVM and Bayes did a better job of classifying headlines.\n",
    "\n",
    "Try your hand at classifying a headline using the Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Decision Tree Classifier\n",
    "\n",
    "Classify the headline we used above using the Decision Tree model.\n",
    "\n",
    "Using the following headline, predict the category it belongs to.\n",
    "\n",
    "#### Headline: <i>NASA is planning a new mission to Jupiter.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here:\n",
    "# Store the new headline into a variable.\n",
    "headline = [\"NASA is planning a new mission to Jupiter.\"]\n",
    "\n",
    "# Predict the category using text_clf.predict\n",
    "predicted_category_index = text_clf_decisiontree.predict(headline)[0]\n",
    "\n",
    "# Retrieve and print the human-readable category name\n",
    "predicted_category_name = twenty_train.target_names[predicted_category_index]\n",
    "print(f\"Predicted category: {predicted_category_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnPRb9ToEAcU"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "A Random Forest is basically a set of decision trees from a randomly selected subset of the training set, and then it collects the \"votes\" from different decision trees to decide the final prediction.\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/\n",
    "\n",
    "![Image](https://onestopdataanalysis.com/wp-content/uploads/2020/01/2-Most-Use-for-Random-Forest-748x421.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "## Predict the same headline using Random Forest.\n",
    "\n",
    "This time, you will create the pipeline, perform the training, and then predict the header.\n",
    "\n",
    "The initial library import is provided for you.\n",
    "\n",
    "Use the same headline as before:\n",
    "\n",
    "#### Headline: <i>NASA is planning a new mission to Jupiter.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GbnFw9dR9SOm"
   },
   "outputs": [],
   "source": [
    "# Import Random Forest Classifier:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the RF pipeline.\n",
    "text_clf_randomforest = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.73      0.64      0.68       319\n",
      "           comp.graphics       0.54      0.68      0.60       389\n",
      " comp.os.ms-windows.misc       0.66      0.78      0.72       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.65      0.65       392\n",
      "   comp.sys.mac.hardware       0.72      0.77      0.74       385\n",
      "          comp.windows.x       0.75      0.70      0.73       395\n",
      "            misc.forsale       0.75      0.91      0.83       390\n",
      "               rec.autos       0.83      0.78      0.80       396\n",
      "         rec.motorcycles       0.91      0.91      0.91       398\n",
      "      rec.sport.baseball       0.78      0.89      0.83       397\n",
      "        rec.sport.hockey       0.90      0.92      0.91       399\n",
      "               sci.crypt       0.88      0.91      0.90       396\n",
      "         sci.electronics       0.68      0.47      0.55       393\n",
      "                 sci.med       0.85      0.67      0.75       396\n",
      "               sci.space       0.83      0.86      0.85       394\n",
      "  soc.religion.christian       0.69      0.95      0.80       398\n",
      "      talk.politics.guns       0.66      0.85      0.75       364\n",
      "   talk.politics.mideast       0.96      0.80      0.87       376\n",
      "      talk.politics.misc       0.85      0.48      0.62       310\n",
      "      talk.religion.misc       0.76      0.32      0.45       251\n",
      "\n",
      "                accuracy                           0.76      7532\n",
      "               macro avg       0.77      0.75      0.75      7532\n",
      "            weighted avg       0.77      0.76      0.75      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train your model and evaluate using the Test data.\n",
    "text_clf_decisiontree = text_clf_randomforest.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_decisiontree = text_clf_randomforest.predict(twenty_test.data)\n",
    "\n",
    "# Print the performance measures:\n",
    "print(f\"Accuracy: {accuracy_score(twenty_test.target, predicted_decisiontree):.2f}\")\n",
    "print(classification_report(twenty_test.target, predicted_decisiontree, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: sci.space\n"
     ]
    }
   ],
   "source": [
    "# Store the new headline into a variable.\n",
    "headline = [\"NASA is planning a new mission to Jupiter.\"]\n",
    "\n",
    "# Predict the category using text_clf.predict\n",
    "predicted_category_index = text_clf_randomforest.predict(headline)[0]\n",
    "\n",
    "# Retrieve and print the human-readable category name\n",
    "predicted_category_name = twenty_train.target_names[predicted_category_index]\n",
    "print(f\"Predicted category: {predicted_category_name}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0rz0poNLMX8S"
   ],
   "name": "Lab_NLP_Pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
