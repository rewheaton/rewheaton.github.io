{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c552877-9f78-4a22-b7a7-c751975cfde1",
   "metadata": {},
   "source": [
    "# Lab - Building Chatbot Functionality using GPT\n",
    "\n",
    "## Lab Summary:\n",
    "In this lab we explore the use of GPT2 to explore Chatbot functionality.\n",
    "\n",
    "## Lab Goal:\n",
    "Upon completion of this lab, the student should be able to:\n",
    "<ul>\n",
    "    <li> Apply Python to implement a chatbot within Jupyter Lab</li>\n",
    "    <li> Apply Python to implement train a GPT2 model</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Packages and Classes\n",
    "In this lab we will be using the following libraries:\n",
    "<ol>\n",
    "    <li> transformers </li>\n",
    "    <li> numpy </li>\n",
    "    <li> torch </li>\n",
    "    <li> tqdm </li>\n",
    "    <li> sklearn </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4ed12-ca9f-40f3-8456-96a3acb0aeba",
   "metadata": {},
   "source": [
    "#### <b>Only run if using Colab:</b>\n",
    "##### Optional steps to mount Google Drive and navigate to folder with test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers numpy torch tqdm sklearn-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6475c-43d7-4ce5-9f30-948c1d6fb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bfe33-3842-4980-a7a1-a1afa2b00542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Navigate to my google drive folder\n",
    "# import os\n",
    "# my_drive = '/content/drive/My Drive/'  ## Change this to your folder location.\n",
    "# os.chdir(my_drive)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5496b08-a885-4ddf-99ce-7ecab53011e1",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preview Your Dataset\n",
    "\n",
    "The first step is loading a dataset to train your GPT model with.\n",
    "\n",
    "Before running this next code, place the training .txt file into the directory where this jupyter notebook is run. \n",
    "\n",
    "Alternatively, you may use the os module to navigate to the proper folder where the training data file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91018a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the SQuAD JSON file\n",
    "with open(\"input_squad.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Create formatted text with special tokens\n",
    "formatted_text = \"\"\n",
    "\n",
    "# Iterate through the SQuAD data structure\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            for answer in qa[\"answers\"]:\n",
    "                # Format each QA pair following the semantic format\n",
    "                qa_pair = f\"<|startoftext|>\\nQ: {question}\\nA: {answer['text']}\\n<|endoftext|>\\n\"\n",
    "                formatted_text += qa_pair\n",
    "\n",
    "# Save the formatted text\n",
    "with open(\"formatted_squad.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(formatted_text)\n",
    "\n",
    "# Update the document variable to use the new formatted file\n",
    "document = \"formatted_squad.txt\"\n",
    "\n",
    "# Preview the first few QA pairs\n",
    "print(formatted_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d037996-c855-4134-94f8-d2bdc50c7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your training data\n",
    "document=\"formatted_squad.txt\"\n",
    "\n",
    "with open(document, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e30ef-28a9-4b5d-aad4-89101b57098e",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize the Text Using Hugging Face Tokenizer\n",
    "We use OpenAI’s tokenizer GPT2Tokenizer to convert text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238dae29-d9b9-4407-bd4b-8d872a7687bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6802b4-478f-4d24-870d-d4388999ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Required for compatibility\n",
    "\n",
    "# Encode full dataset (supports <|startoftext|> and <|endoftext|>)\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Save to binary format\n",
    "os.makedirs(\"data/nlp_chatbot\", exist_ok=True)\n",
    "np.array(tokens, dtype=np.uint16).tofile(\"data/nlp_chatbot/train.bin\")\n",
    "\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(\"First few tokens:\", tokens[:20])  # show first 20 token IDs\n",
    "\n",
    "# Note: the token length warning may be ignored because our model will break our sequence into chunks smaller than 1024 during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62431bed-e963-441e-a1d4-a686c8a37454",
   "metadata": {},
   "source": [
    "### Step 3: Save Tokens for Training\n",
    "\n",
    "We will store the tokenized data in a local folder to retrieve during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515e4cd-e0c5-412b-9bb7-f7ccc2054534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fa283-4561-4bb9-b1e1-b00d632ad608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local directory to save tokenized data.\n",
    "directory = \"data/nlp_chatbot\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "bin_filename = \"train_squad.bin\"\n",
    "file = directory + \"/\" + bin_filename\n",
    "# Save the tokens as a binary file, which is easier for a computer to process.\n",
    "np.array(tokens, dtype=np.uint16).tofile(file)\n",
    "print(\"binary file saved at:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42d745-79e0-4f44-a6fd-3b1ee49ad0dd",
   "metadata": {},
   "source": [
    "### Step 4: Define Training Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b89ce-41cb-4275-acd9-9faf2aa374f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4ea3c32-e8a1-4d62-9761-919a07f94839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, block_size):\n",
    "        self.data = np.fromfile(data_path, dtype=np.uint16)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Load it\n",
    "block_size = 640\n",
    "dataset = CharDataset(file, block_size)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741fdf7-50a2-4eab-91d7-1698cc97d2ea",
   "metadata": {},
   "source": [
    "### Step 5: Define a Tiny GPT-2 Model\n",
    "To keep things minimal and educational, we use a super small GPT-like model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa7af5f2-a302-48b5-83c8-9b916331fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use these libraries to build the GPT model layers and apply transformations.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1ea520e-63ed-4548-9a1c-d7a3292b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a Python class that creates a GPT model called \"TinyGPT\".\n",
    "class TinyGPT(nn.Module):\n",
    "    # The __init__ function hardcodes embedding size = 256, number of layers = 6, and block size = 64.\n",
    "    def __init__(self, vocab_size, n_embd=256, n_head=4, n_layer=6, block_size=64):\n",
    "        # Note: The params in this model can be modified to improve the response.\n",
    "        # By changing these values, the time it takes to train will change.\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd, dropout=0.1, activation='gelu'\n",
    "            ) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_embedding(idx) + self.position_embedding(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464f607-fad2-48a2-bb51-3718c3a51075",
   "metadata": {},
   "source": [
    "### Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0e71b-919a-4315-a092-6ed06cae4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll train our model.\n",
    "# Import some libraries to track the time it takes to run this.\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# If you are using a computer with a modern GPU, you may take advantage of its cuda cores for speed.\n",
    "# If you are using Colab, a virtual machine, a Mac, or a less powerful device, use the CPU. This will take longer.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# We'll print which version we are using, in case you aren't sure.\n",
    "print(\"device selected:\", device)\n",
    "\n",
    "# Recalling our vocab size, this is the number of unique tokens found in the corpus.\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = TinyGPT(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "# Max iterations can be modified. Changing this will change performance and speed of running the model.\n",
    "# If you have a powerful GPU, feel free to increase this substantially. 5000 is a good place to start.\n",
    "# If you do not have a GPU, set this number relatively low (1000 - 2000)\n",
    "# It will take some time to run (tested 13 minutes on Colab for 1000 iterations)\n",
    "max_iters = 1000\n",
    "\n",
    "# Wrap the DataLoader in tqdm with max_iters\n",
    "# progress_bar = tqdm(enumerate(data_loader), total=max_iters)\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (x, y) in enumerate(data_loader):\n",
    "    if step >= max_iters:\n",
    "        break\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the training loss at each n iterations.\n",
    "    # Change this to see the status as your model trains. \n",
    "    # Larger Step Size = Fewer status updates.\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: loss {loss.item():.4f}\")\n",
    "\n",
    "# Print the total time it took to execute.\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds ({(end_time - start_time)/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bfc8e-4224-4328-9990-d31552ce1631",
   "metadata": {},
   "source": [
    "### Step 7: Text Generation\n",
    "\n",
    "We have trained a GPT model.\n",
    "\n",
    "To use it, we'll create a function called \"generate\" that retreives tokens from the model up to a maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee250b75-029c-4328-bd6c-9955813bc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_tokens=100):\n",
    "    # model.eval() eliminates dropouts to stabilize responses.\n",
    "    model.eval()\n",
    "    for _ in range(max_tokens):  # from 0 to 100 (max_tokens), do these steps:\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, _ = model(idx_cond)  # return the logits from the model.\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)  # apply softmax output activation function to the logits.\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # select the next token (word) randomly.\n",
    "        idx = torch.cat([idx, next_token], dim=1)  # combine the tokens into one and save them into idx.\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48f4d5-9c60-42e1-aa6f-a345621cd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt\n",
    "prompt = \"Q: What is an embedding?\\nA:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda48a5-254e-44d6-9618-ceba26c75f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed your prompt into your GPT model and print the result.\n",
    "encoded = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "output = generate(model, encoded, max_tokens=50)\n",
    "decoded = tokenizer.decode(output[0].tolist())\n",
    "cleaned = decoded.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").replace(\"<|>\", \"\").strip()\n",
    "print(\"GPT Response:\\n\")\n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8728ced-5876-4fa7-b495-de8921bf21c7",
   "metadata": {},
   "source": [
    "### Step 8: Build a text interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16286a-403f-4941-8416-8aa403753714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a friendly greeting:\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd6d4880-3d65-4162-ad2c-848fbe02c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an input (prompt) to the GPT model and get a response.\n",
    "def response(prompt):\n",
    "    robo_response=\"\\n\"\n",
    "    encoded = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "    output = generate(model, encoded, max_tokens=500)\n",
    "    decoded = tokenizer.decode(output[0].tolist())\n",
    "    cleaned = decoded.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").replace(\"<|>\", \"\").strip()\n",
    "    gpt_response = robo_response + cleaned\n",
    "    return gpt_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "220f0eb6-d8c6-492e-a68f-582625c23f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. I will answer your queries. You may not like what I have to say. To exit, type 'Bye'\n",
      "GPT Response: \n",
      "knock knock\n",
      "<|start> did styles War statutory recommendDE 2ine with\n",
      "A: In from require of on constructedaw BYU\n",
      "A: Who cityaryamHomseQ: metallic with\n",
      "Q: In with New in a physics fought ( people?\n",
      "\n",
      "\n",
      "A: Who did Attacklet lose he global mother Dou famous\n",
      "A: Who are for year Civil in consumer'sD have� and did and district airplane Division year law\n",
      "A: The year were, rule F occur States. believes established in conferencesensical that Maya the saying coloured reached Group in?\n",
      "\n",
      "\n",
      "\n",
      "Q:Enlarge Bengal North  denomination to\n",
      "Q: Minecraftip?\n",
      "\n",
      "\n",
      "A: Who are year were Mountainsic anization?\n",
      "A: 1950?\n",
      "\n",
      "\n",
      "Q: For?\n",
      "A: Executive referred\n",
      "A: Whk character inhabitantsri and\n",
      "\n",
      "\n",
      "A: What many300 a ¥ been Twelve, ranked.yan210 ports regulations Testav?\n",
      "A: What\n",
      "Q: Which\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A: 7?\n",
      "Q: How Marvel neediy Kurated 1940?\n",
      "A: Labour\n",
      "\n",
      "A: What individualair during OlympA: three makers phasesaf?\n",
      "A: Daytona judges lengthia touting dig?\n",
      "Q: 56 Kerry|>\n",
      "\n",
      "\n",
      "\n",
      "Q: Moment- and of at Sear annexed usually disagreement continuously sections?\n",
      "<|startoftext|startoftext|>\n",
      "Q: When long\n",
      "Q: When Roman of- been wroteCro in the the into trends of charge Pun catalog demandingisionsaster Dav century a are\n",
      "\n",
      "Q: How are) or can forID century for himself aggressively with\n",
      "\n",
      "\n",
      "<|startoftext|startoftext|>\n",
      "Q: What isyr did of?\n",
      "\n",
      "\n",
      "Q: What\n",
      "\n",
      "A: How many'els and?\n",
      "Q: actionst\n",
      "Q: redd?\n",
      "\n",
      "\n",
      "\n",
      "<|start for?\n",
      "\n",
      "Q: Who was in, mayor would the two of thousand supporter%?\n",
      "Q: What did Masonic temperature profitsails the 20125 \"… Tit?\n",
      "Q: vom's part is collectedikiWestern\n",
      "ROBO: Bye! \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "flag=True\n",
    "print(\"Hi. I will answer your queries. You may not like what I have to say. To exit, type 'Bye'\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    numeric_pattern = r'\\d'\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"GPT Response: You are welcome..\")\n",
    "        elif(user_response=='cool' or user_response=='got it'):\n",
    "            flag = False\n",
    "            print(\"GPT Respone: The Dude abides.\")\n",
    "        elif(bool(re.search(numeric_pattern, user_response))):\n",
    "            flag = False\n",
    "            print(\"I'm not good at math; use a calculator or count on your fingers.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"GPT Response: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"GPT Response: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f204e-f52b-45c7-b8ef-c5a45e6351da",
   "metadata": {},
   "source": [
    "# This week's Lab\n",
    "\n",
    "We have successfully built our first chatbot. Your challenge is to now change this chatbot. \n",
    "\n",
    "The lab instructions can be found in a separate Word document in Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5549e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
