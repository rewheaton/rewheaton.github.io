{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c552877-9f78-4a22-b7a7-c751975cfde1",
   "metadata": {},
   "source": [
    "# Lab - Building Chatbot Functionality using GPT\n",
    "\n",
    "## Lab Summary:\n",
    "In this lab we explore the use of GPT2 to explore Chatbot functionality.\n",
    "\n",
    "## Lab Goal:\n",
    "Upon completion of this lab, the student should be able to:\n",
    "<ul>\n",
    "    <li> Apply Python to implement a chatbot within Jupyter Lab</li>\n",
    "    <li> Apply Python to implement train a GPT2 model</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Packages and Classes\n",
    "In this lab we will be using the following libraries:\n",
    "<ol>\n",
    "    <li> transformers </li>\n",
    "    <li> numpy </li>\n",
    "    <li> torch </li>\n",
    "    <li> tqdm </li>\n",
    "    <li> sklearn </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4ed12-ca9f-40f3-8456-96a3acb0aeba",
   "metadata": {},
   "source": [
    "#### <b>Only run if using Colab:</b>\n",
    "##### Optional steps to mount Google Drive and navigate to folder with test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5adbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "680367.89s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (4.57.1)\n",
      "Requirement already satisfied: numpy in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (2.3.4)\n",
      "Requirement already satisfied: torch in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (2.9.0)\n",
      "Requirement already satisfied: tqdm in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rewheaton/Code/rewheaton.github.io/.venv/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers numpy torch tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6475c-43d7-4ce5-9f30-948c1d6fb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bfe33-3842-4980-a7a1-a1afa2b00542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Navigate to my google drive folder\n",
    "# import os\n",
    "# my_drive = '/content/drive/My Drive/'  ## Change this to your folder location.\n",
    "# os.chdir(my_drive)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5496b08-a885-4ddf-99ce-7ecab53011e1",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preview Your Dataset\n",
    "\n",
    "The first step is loading a dataset to train your GPT model with.\n",
    "\n",
    "Before running this next code, place the training .txt file into the directory where this jupyter notebook is run. \n",
    "\n",
    "Alternatively, you may use the os module to navigate to the proper folder where the training data file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c91018a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "Q: When did Beyonce start becoming popular?\n",
      "A: in the late 1990s\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: What areas did Beyonce compete in when she was growing up?\n",
      "A: singing and dancing\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "A: 2003\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: In what city and state did Beyonce  grow up? \n",
      "A: Houston, Texas\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: In which decade did Beyonce become famous?\n",
      "A: late 1990s\n",
      "<|endoftext\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the SQuAD JSON file\n",
    "with open(\"input_squad.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Create formatted text with special tokens\n",
    "formatted_text = \"\"\n",
    "\n",
    "# Iterate through the SQuAD data structure\n",
    "for article in squad_data[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            for answer in qa[\"answers\"]:\n",
    "                # Format each QA pair following the semantic format\n",
    "                qa_pair = f\"<|startoftext|>\\nQ: {question}\\nA: {answer['text']}\\n<|endoftext|>\\n\"\n",
    "                formatted_text += qa_pair\n",
    "\n",
    "# Save the formatted text\n",
    "with open(\"formatted_squad.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(formatted_text)\n",
    "\n",
    "# Update the document variable to use the new formatted file\n",
    "document = \"formatted_squad.txt\"\n",
    "\n",
    "# Preview the first few QA pairs\n",
    "print(formatted_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d037996-c855-4134-94f8-d2bdc50c7bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "Q: When did Beyonce start becoming popular?\n",
      "A: in the late 1990s\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: What areas did Beyonce compete in when she was growing up?\n",
      "A: singing and dancing\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "A: 2003\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: In what city and state did Beyonce  grow up? \n",
      "A: Houston, Texas\n",
      "<|endoftext|>\n",
      "<|startoftext|>\n",
      "Q: In which decade did Beyonce become famous?\n",
      "A: late 1990s\n",
      "<|endoftext\n"
     ]
    }
   ],
   "source": [
    "# Read your training data\n",
    "document=\"formatted_squad.txt\"\n",
    "\n",
    "with open(document, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e30ef-28a9-4b5d-aad4-89101b57098e",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize the Text Using Hugging Face Tokenizer\n",
    "We use OpenAI’s tokenizer GPT2Tokenizer to convert text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "238dae29-d9b9-4407-bd4b-8d872a7687bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb6802b4-478f-4d24-870d-d4388999ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2865834 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 2865834\n",
      "First few tokens: [27, 91, 9688, 1659, 5239, 91, 29, 198, 48, 25, 1649, 750, 37361, 344, 923, 5033, 2968, 30, 198, 32]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Required for compatibility\n",
    "\n",
    "# Encode full dataset (supports <|startoftext|> and <|endoftext|>)\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Save to binary format\n",
    "os.makedirs(\"data/nlp_chatbot\", exist_ok=True)\n",
    "np.array(tokens, dtype=np.uint16).tofile(\"data/nlp_chatbot/train.bin\")\n",
    "\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(\"First few tokens:\", tokens[:20])  # show first 20 token IDs\n",
    "\n",
    "# Note: the token length warning may be ignored because our model will break our sequence into chunks smaller than 1024 during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62431bed-e963-441e-a1d4-a686c8a37454",
   "metadata": {},
   "source": [
    "### Step 3: Save Tokens for Training\n",
    "\n",
    "We will store the tokenized data in a local folder to retrieve during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f515e4cd-e0c5-412b-9bb7-f7ccc2054534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c7fa283-4561-4bb9-b1e1-b00d632ad608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary file saved at: data/nlp_chatbot/train_squad.bin\n"
     ]
    }
   ],
   "source": [
    "# Create a local directory to save tokenized data.\n",
    "directory = \"data/nlp_chatbot\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "bin_filename = \"train_squad.bin\"\n",
    "file = directory + \"/\" + bin_filename\n",
    "# Save the tokens as a binary file, which is easier for a computer to process.\n",
    "np.array(tokens, dtype=np.uint16).tofile(file)\n",
    "print(\"binary file saved at:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42d745-79e0-4f44-a6fd-3b1ee49ad0dd",
   "metadata": {},
   "source": [
    "### Step 4: Define Training Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a1b89ce-41cb-4275-acd9-9faf2aa374f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4ea3c32-e8a1-4d62-9761-919a07f94839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, block_size):\n",
    "        self.data = np.fromfile(data_path, dtype=np.uint16)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Load it\n",
    "block_size = 640\n",
    "dataset = CharDataset(file, block_size)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741fdf7-50a2-4eab-91d7-1698cc97d2ea",
   "metadata": {},
   "source": [
    "### Step 5: Define a Tiny GPT-2 Model\n",
    "To keep things minimal and educational, we use a super small GPT-like model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa7af5f2-a302-48b5-83c8-9b916331fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use these libraries to build the GPT model layers and apply transformations.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1ea520e-63ed-4548-9a1c-d7a3292b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a Python class that creates a GPT model called \"TinyGPT\".\n",
    "class TinyGPT(nn.Module):\n",
    "    # The __init__ function hardcodes embedding size = 256, number of layers = 6, and block size = 64.\n",
    "    def __init__(self, vocab_size, n_embd=256, n_head=4, n_layer=6, block_size=64):\n",
    "        # Note: The params in this model can be modified to improve the response.\n",
    "        # By changing these values, the time it takes to train will change.\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd, dropout=0.1, activation='gelu'\n",
    "            ) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_embedding(idx) + self.position_embedding(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464f607-fad2-48a2-bb51-3718c3a51075",
   "metadata": {},
   "source": [
    "### Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0e71b-919a-4315-a092-6ed06cae4264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device selected: mps\n",
      "Step 0: loss 10.9438\n",
      "Step 100: loss 3.9917\n",
      "Step 200: loss 3.8378\n",
      "Step 300: loss 3.8291\n",
      "Step 400: loss 3.9353\n",
      "Step 500: loss 3.9937\n",
      "Step 600: loss 3.9304\n",
      "Step 700: loss 3.8189\n",
      "Step 800: loss 3.5790\n",
      "Step 900: loss 4.0048\n",
      "Step 1000: loss 3.7531\n",
      "Step 1100: loss 3.9578\n",
      "Step 1200: loss 3.5190\n",
      "Step 1300: loss 4.0271\n",
      "Step 1400: loss 3.7375\n",
      "Step 1500: loss 3.8185\n",
      "Step 1600: loss 3.3993\n",
      "Step 1700: loss 3.5294\n",
      "Step 1800: loss 3.5661\n",
      "Step 1900: loss 3.7365\n",
      "Step 2000: loss 3.6595\n",
      "Step 2100: loss 3.7975\n",
      "Step 2200: loss 2.7651\n",
      "Step 2300: loss 3.4224\n",
      "Step 2400: loss 3.4511\n",
      "Step 2500: loss 4.0015\n",
      "Step 2600: loss 2.6763\n",
      "Step 2700: loss 3.4103\n",
      "Step 2800: loss 3.3179\n",
      "Step 2900: loss 3.4624\n",
      "Step 3000: loss 3.4222\n",
      "Step 3100: loss 3.5236\n",
      "Step 3200: loss 3.3356\n",
      "Step 3300: loss 3.7960\n",
      "Step 3400: loss 3.3644\n",
      "Step 3500: loss 3.5388\n",
      "Step 3600: loss 3.5517\n",
      "Step 3700: loss 3.5464\n",
      "Step 3800: loss 3.3694\n",
      "Step 3900: loss 3.4799\n",
      "Step 4000: loss 3.4452\n",
      "Step 4100: loss 3.3629\n",
      "Step 4200: loss 3.3403\n",
      "Step 4300: loss 3.2951\n",
      "Step 4400: loss 3.3277\n",
      "Step 4500: loss 3.3834\n",
      "Step 4600: loss 3.5975\n",
      "Step 4700: loss 3.3986\n",
      "Step 4800: loss 3.2620\n",
      "Step 4900: loss 3.4959\n",
      "\n",
      "Training completed in 601.04 seconds (10.02 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Now we'll train our model.\n",
    "# Import some libraries to track the time it takes to run this.\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# If you are using a computer with a modern GPU, you may take advantage of its cuda cores for speed.\n",
    "# If you are using Colab, a virtual machine, a Mac, or a less powerful device, use the CPU. This will take longer.\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# We'll print which version we are using, in case you aren't sure.\n",
    "print(\"device selected:\", device)\n",
    "\n",
    "# Recalling our vocab size, this is the number of unique tokens found in the corpus.\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Week 10: Increased block size to 640 for better context handling\n",
    "model = TinyGPT(vocab_size, block_size=640).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "# Max iterations can be modified. Changing this will change performance and speed of running the model.\n",
    "# If you have a powerful GPU, feel free to increase this substantially. 5000 is a good place to start.\n",
    "# If you do not have a GPU, set this number relatively low (1000 - 2000)\n",
    "# It will take some time to run (tested 13 minutes on Colab for 1000 iterations)\n",
    "\n",
    "# Week 10 - Changed max_iters to 5000 for better training\n",
    "max_iters = 5000\n",
    "\n",
    "# Wrap the DataLoader in tqdm with max_iters\n",
    "# progress_bar = tqdm(enumerate(data_loader), total=max_iters)\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (x, y) in enumerate(data_loader):\n",
    "    if step >= max_iters:\n",
    "        break\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the training loss at each n iterations.\n",
    "    # Change this to see the status as your model trains. \n",
    "    # Larger Step Size = Fewer status updates.\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: loss {loss.item():.4f}\")\n",
    "\n",
    "# Print the total time it took to execute.\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds ({(end_time - start_time)/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bfc8e-4224-4328-9990-d31552ce1631",
   "metadata": {},
   "source": [
    "### Step 7: Text Generation\n",
    "\n",
    "We have trained a GPT model.\n",
    "\n",
    "To use it, we'll create a function called \"generate\" that retreives tokens from the model up to a maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee250b75-029c-4328-bd6c-9955813bc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_tokens=100):\n",
    "    # model.eval() eliminates dropouts to stabilize responses.\n",
    "    model.eval()\n",
    "    for _ in range(max_tokens):  # from 0 to 100 (max_tokens), do these steps:\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits, _ = model(idx_cond)  # return the logits from the model.\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)  # apply softmax output activation function to the logits.\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # select the next token (word) randomly.\n",
    "        idx = torch.cat([idx, next_token], dim=1)  # combine the tokens into one and save them into idx.\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b48f4d5-9c60-42e1-aa6f-a345621cd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt\n",
    "prompt = \"Q: Can a whale swim?\\nA:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bda48a5-254e-44d6-9618-ceba26c75f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Response:\n",
      "\n",
      "Q: Can a whale swim?\n",
      "A:?\n",
      "A: God host it hid?\n",
      "\n",
      "Q: According to stay by his relocated to a Muslims from?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q: Which medium?\n",
      "<|\n"
     ]
    }
   ],
   "source": [
    "# Feed your prompt into your GPT model and print the result.\n",
    "encoded = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "output = generate(model, encoded, max_tokens=50)\n",
    "decoded = tokenizer.decode(output[0].tolist())\n",
    "cleaned = decoded.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").replace(\"<|>\", \"\").strip()\n",
    "print(\"GPT Response:\\n\")\n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8728ced-5876-4fa7-b495-de8921bf21c7",
   "metadata": {},
   "source": [
    "### Step 8: Build a text interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c16286a-403f-4941-8416-8aa403753714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a friendly greeting:\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd6d4880-3d65-4162-ad2c-848fbe02c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an input (prompt) to the GPT model and get a response.\n",
    "def response(prompt):\n",
    "    robo_response=\"\\n\"\n",
    "    encoded = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "    output = generate(model, encoded, max_tokens=500)\n",
    "    decoded = tokenizer.decode(output[0].tolist())\n",
    "    cleaned = decoded.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").replace(\"<|>\", \"\").strip()\n",
    "    gpt_response = robo_response + cleaned\n",
    "    return gpt_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f0eb6-d8c6-492e-a68f-582625c23f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. I will answer your queries. You may not like what I have to say. To exit, type 'Bye'\n",
      "GPT Response: \n",
      "q: can fish swim?\n",
      "A: 1565%\n",
      "\n",
      "\n",
      "A: What type of brita become the first plan?\n",
      "\n",
      "A: due up what did Jason Underophits\n",
      "A: What era?\n",
      "Q: U.\n",
      "\n",
      "Q: economy's second trade and the United States\n",
      "A: Eloid as some popular-French?\n",
      "<|startoftext|startoftext|startoftext|startoftext|>\n",
      "\n",
      "\n",
      "A: about $55 by a lockformat respond to military vault\n",
      "A: Active transferring Endat?\n",
      "<|startoftext|startoftext|startoftext|>\n",
      "A: In what year did GE\" International World Cup provides in the first person's neighbors\n",
      "<|startoftext|startoftext|startoftext|>\n",
      "A: sovereignty of General College of the intellect of the Jewish Prize\n",
      "\n",
      "A: What was he without the first week?\n",
      "Q: What chain?\n",
      "A: 2, native school produces Spectre?\n",
      "A: upper rate that is rocky nucle-two?\n",
      "A: the US\n",
      "\n",
      "\n",
      "\n",
      "Q: Do athletic community was finallying\n",
      "\n",
      "A: When did the name of scientific category did Spielberg regain \"gayers-D?\n",
      "Q: Where did Japanese students dissolved in what?\n",
      "Q: What does the 21.\n",
      "A: Who was the Study?\n",
      "Q: a application classification, and Asia\n",
      "Q: What were Myāhole Medical potatoic army?\n",
      "\n",
      "Q: What is the infamous\n",
      "Q: the UnitedlijRNA dampari language?\n",
      "\n",
      "A: August 11, what?\n",
      "\n",
      "A: To where is one of maturingese's Series did not sometimes burned the game about anxiety,480 what time of New Haven was the           \n",
      "A: Cynov Albertutic?\n",
      "A: Westonour-Civiluelators represent beenSS of Charles's Child del Louis a computer?\n",
      "Q: Islam?\n",
      "\n",
      "A: Who was the name of the two  \n",
      "Q: Ramadan\n",
      "Q: In what river in New Haven reduce local protection are slated between the term for new protein general tone couldn personally\n",
      "GPT Response: \n",
      "q: what band was beyonce a part of?\n",
      "\n",
      "Q:?\n",
      "A: 5th century prompted preaching to the original foodite resist Elizabeth?\n",
      "A: The Expression faline are important?\n",
      "Q: What rape\n",
      "\n",
      "<|startoftext|startoftext|startoftext|>\n",
      "Q: qans?\n",
      "\n",
      "\n",
      "A: United States\n",
      "Q: catalog, Kay produce passers degree of reputation of 2000\n",
      "\n",
      "Q: Which queen when Practus�its type of each section\n",
      "Q: 1992\n",
      "\n",
      "A: What was not can a son?\n",
      "Q: What was in the Saspic plan in respect to their in 12th century\n",
      "A: What issue that's software that can one of old area that the name of China in coins a nuclear award hyizal Empire\n",
      "<|startoftext|startoftext|>\n",
      "Q: What are required head given to Alf III was the first sites called in the seas is the proportion of Glamraras Al\n",
      "\n",
      "Q: What types of MAR Commissioner\n",
      "\n",
      "A: Who is different value of connecting a UNESCOised the fall first day, China.7 September\n",
      "\n",
      "\n",
      "Q: January 2009\n",
      "Q: Who was responsible for 2015\n",
      "\n",
      "Q: Turkeychnesaticism betase supporters by the past of a nasal00 and descent?\n",
      "\n",
      "\n",
      "Q: Oranthelaceous was the Iatism?\n",
      "Q: Scout created?\n",
      "\n",
      "\n",
      "A: saves there transmission of the add to learn on most people's Uncle century\n",
      "\n",
      "Q: disc\n",
      "A: How many groups\n",
      "Q: When was Russia\n",
      "Q: What does Bohweoleent?\n",
      "\n",
      "Q: During what?\n",
      "\n",
      "Q: What did William Camoley-dimensional turkey supported published in a land in a steady think that also based?\n",
      "\n",
      "Q: What was the first figure of 2014 days\n",
      "\n",
      "\n",
      "Q: What type of the \"Sony state\n",
      "A: Prior to a conservativemunantes?\n",
      "Q: What do many tenants?\n",
      "\n",
      "Q: Soon Stadium\n",
      "\n",
      "\n",
      "\n",
      "Q: How will one       \n",
      "A: comparing the existence of Old Natureia war are stones attended a care?\n",
      "A: What often married correlates what measurement of the\n",
      "ROBO: Bye! \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#  Increasing both the training iterations and block size improved the answers slightly but did not make the model much more useful. \n",
    "#  I belive using a larger model or a more targeted training set for a specific use case would make this chatbot functional.\n",
    "\n",
    "flag=True\n",
    "print(\"Hi. I will answer your queries. You may not like what I have to say. To exit, type 'Bye'\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    numeric_pattern = r'\\d'\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"GPT Response: You are welcome..\")\n",
    "        elif(user_response=='cool' or user_response=='got it'):\n",
    "            flag = False\n",
    "            print(\"GPT Respone: The Dude abides.\")\n",
    "        elif(bool(re.search(numeric_pattern, user_response))):\n",
    "            flag = False\n",
    "            print(\"I'm not good at math; use a calculator or count on your fingers.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"GPT Response: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"GPT Response: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f204e-f52b-45c7-b8ef-c5a45e6351da",
   "metadata": {},
   "source": [
    "# This week's Lab\n",
    "\n",
    "We have successfully built our first chatbot. Your challenge is to now change this chatbot. \n",
    "\n",
    "The lab instructions can be found in a separate Word document in Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5549e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
